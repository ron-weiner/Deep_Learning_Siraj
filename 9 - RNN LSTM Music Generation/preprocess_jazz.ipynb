{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author:     Ji-Sung Kim\n",
    "Project:    deepjazz\n",
    "Purpose:    Parse, cleanup and process data.\n",
    "\n",
    "Code adapted from Evan Chow's jazzml, https://github.com/evancchow/jazzml with\n",
    "express permission.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from music21 import *\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import groupby, zip_longest\n",
    "from grammar import *\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "#----------------------------HELPER FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Helper function to parse a MIDI file into its measures and chords '''\n",
    "def __parse_midi(data_fn):\n",
    "    # Parse the MIDI data for separate melody and accompaniment parts.\n",
    "    midi_data = converter.parse(data_fn)\n",
    "    # Get melody part, compress into single voice.\n",
    "    melody_stream = midi_data[5]     # For Metheny piece, Melody is Part #5.\n",
    "    melody1, melody2 = melody_stream.getElementsByClass(stream.Voice)\n",
    "    for j in melody2:\n",
    "        melody1.insert(j.offset, j)\n",
    "    melody_voice = melody1\n",
    "\n",
    "    for i in melody_voice:\n",
    "        if i.quarterLength == 0.0:\n",
    "            i.quarterLength = 0.25\n",
    "\n",
    "    # Change key signature to adhere to comp_stream (1 sharp, mode = major).\n",
    "    # Also add Electric Guitar. \n",
    "    melody_voice.insert(0, instrument.ElectricGuitar())\n",
    "    melody_voice.insert(0, key.KeySignature(sharps=1))\n",
    "\n",
    "    # The accompaniment parts. Take only the best subset of parts from\n",
    "    # the original data. Maybe add more parts, hand-add valid instruments.\n",
    "    # Should add least add a string part (for sparse solos).\n",
    "    # Verified are good parts: 0, 1, 6, 7 '''\n",
    "    partIndices = [0, 1, 6, 7]\n",
    "    comp_stream = stream.Voice()\n",
    "    comp_stream.append([j.flat for i, j in enumerate(midi_data) \n",
    "        if i in partIndices])\n",
    "\n",
    "    # Full stream containing both the melody and the accompaniment. \n",
    "    # All parts are flattened. \n",
    "    full_stream = stream.Voice()\n",
    "    for i in range(len(comp_stream)):\n",
    "        full_stream.append(comp_stream[i])\n",
    "    full_stream.append(melody_voice)\n",
    "\n",
    "    # Extract solo stream, assuming you know the positions ..ByOffset(i, j).\n",
    "    # Note that for different instruments (with stream.flat), you NEED to use\n",
    "    # stream.Part(), not stream.Voice().\n",
    "    # Accompanied solo is in range [478, 548)\n",
    "    solo_stream = stream.Voice()\n",
    "    for part in full_stream:\n",
    "        curr_part = stream.Part()\n",
    "        curr_part.append(part.getElementsByClass(instrument.Instrument))\n",
    "        curr_part.append(part.getElementsByClass(tempo.MetronomeMark))\n",
    "        curr_part.append(part.getElementsByClass(key.KeySignature))\n",
    "        curr_part.append(part.getElementsByClass(meter.TimeSignature))\n",
    "        curr_part.append(part.getElementsByOffset(476, 548, \n",
    "                                                  includeEndBoundary=True))\n",
    "        cp = curr_part.flat\n",
    "        solo_stream.insert(cp)\n",
    "\n",
    "    # Group by measure so you can classify. \n",
    "    # Note that measure 0 is for the time signature, metronome, etc. which have\n",
    "    # an offset of 0.0.\n",
    "    melody_stream = solo_stream[-1]\n",
    "    measures = OrderedDict()\n",
    "    offsetTuples = [(int(n.offset / 4), n) for n in melody_stream]\n",
    "    measureNum = 0 # for now, don't use real m. nums (119, 120)\n",
    "    for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
    "        measures[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Get the stream of chords.\n",
    "    # offsetTuples_chords: group chords by measure number.\n",
    "    chordStream = solo_stream[0]\n",
    "    chordStream.removeByClass(note.Rest)\n",
    "    chordStream.removeByClass(note.Note)\n",
    "    offsetTuples_chords = [(int(n.offset / 4), n) for n in chordStream]\n",
    "\n",
    "    # Generate the chord structure. Use just track 1 (piano) since it is\n",
    "    # the only instrument that has chords. \n",
    "    # Group into 4s, just like before. \n",
    "    chords = OrderedDict()\n",
    "    measureNum = 0\n",
    "    for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
    "        chords[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Fix for the below problem.\n",
    "    #   1) Find out why len(measures) != len(chords).\n",
    "    #   ANSWER: resolves at end but melody ends 1/16 before last measure so doesn't\n",
    "    #           actually show up, while the accompaniment's beat 1 right after does.\n",
    "    #           Actually on second thought: melody/comp start on Ab, and resolve to\n",
    "    #           the same key (Ab) so could actually just cut out last measure to loop.\n",
    "    #           Decided: just cut out the last measure. \n",
    "    del chords[len(chords) - 1]\n",
    "    try:\n",
    "        assert len(chords) == len(measures)\n",
    "\n",
    "    except AssertionError:\n",
    "        _, _, tb = sys.exc_info()\n",
    "        traceback.print_tb(tb) # Fixed format\n",
    "        tb_info = traceback.extract_tb(tb)\n",
    "        filename, line, func, text = tb_info[-1]\n",
    "        print('An error occurred on line {} in statement {}'.format(line, text))    \n",
    "    \n",
    "    return measures, chords\n",
    "\n",
    "''' Helper function to get the grammatical data from given musical data. '''\n",
    "def __get_abstract_grammars(measures, chords):\n",
    "    # extract grammars\n",
    "    abstract_grammars = []\n",
    "    for ix in range(1, len(measures)):\n",
    "        m = stream.Voice()\n",
    "        for i in measures[ix]:\n",
    "            m.insert(i.offset, i)\n",
    "        c = stream.Voice()\n",
    "        for j in chords[ix]:\n",
    "            c.insert(j.offset, j)\n",
    "        parsed = parse_melody(m, c)\n",
    "        abstract_grammars.append(parsed)\n",
    "\n",
    "    return abstract_grammars\n",
    "\n",
    "#----------------------------PUBLIC FUNCTIONS----------------------------------#\n",
    "\n",
    "''' Get musical data from a MIDI file '''\n",
    "def get_musical_data(data_fn):\n",
    "    measures, chords = __parse_midi(data_fn)\n",
    "    abstract_grammars = __get_abstract_grammars(measures, chords)\n",
    "\n",
    "    return chords, abstract_grammars\n",
    "\n",
    "''' Get corpus data from grammatical data '''\n",
    "def get_corpus_data(abstract_grammars):\n",
    "    corpus = [x for sublist in abstract_grammars for x in sublist.split(' ')]\n",
    "    values = set(corpus)\n",
    "    val_indices = dict((v, i) for i, v in enumerate(values))\n",
    "    indices_val = dict((i, v) for i, v in enumerate(values))\n",
    "\n",
    "    return corpus, values, val_indices, indices_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
