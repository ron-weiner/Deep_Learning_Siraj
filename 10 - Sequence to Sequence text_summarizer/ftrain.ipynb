{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN = 'ftrain'\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import time\n",
    "import fconfig\n",
    "import _pickle as pickle\n",
    "import random\n",
    "import sys\n",
    "import argparse\n",
    "import Levenshtein\n",
    "# os.environ['THEANO_FLAGS'] = 'device=cpu,floatX=float32'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--FN0', default='vocabulary-embedding', help=\"filename of vocab embeddings\")\n",
    "parser.add_argument('--FN1', default='train', help=\"filename of model weights\")\n",
    "parser.add_argument('--batch-size', type=int, default=32, help='input batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs')\n",
    "parser.add_argument('--maxlend', type=int, default=100, help='max length of description')\n",
    "parser.add_argument('--maxlenh', type=int, default=15, help='max length of head')\n",
    "parser.add_argument('--rnn-size', type=int, default=512, help='size of RNN layers')\n",
    "parser.add_argument('--rnn-layers', type=int, default=3, help='number of RNN layers')\n",
    "parser.add_argument('--nsamples', type=int, default=640, help='number of samples per epoch')\n",
    "parser.add_argument('--nflips', type=int, default=0, help='number of flips')\n",
    "parser.add_argument('--temperature', type=float, default=.8, help='RNN temperature')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate, default=0.0001')\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlend = args.maxlend\n",
    "maxlenh = args.maxlenh\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = args.rnn_size\n",
    "rnn_layers = args.rnn_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 10000 10000\n",
      "dimension of embedding space for words: 100\n",
      "vocabulary size: 40,000 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words: 138,281\n",
      "number of words outside vocabulary which we can substitue using glove similarity: 33,731\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov): 64,550\n",
      "\n",
      "Random head, description:\n",
      "H:\n",
      "captured \n",
      "\n",
      "D:\n",
      "at scrapbooking^ top 50 aus , we have a fun tic tac^ toe challenge for you all this month ! ! all you need to do is select any row of items - horizontal , vertical or diagonal^ - and then use the 3 items / techniques on your layout ! our awesome sponsor this month is michelle at scrapbook divas ! ! michelle is offering a $ 20 voucher to our lucky winner ! here is my page i have used leaves , washi^ tape , strip journaling hope you can play along ! \n",
      "\n",
      "0 cls=Embedding name=embedding_1\n",
      "40000x100 \n",
      "\n",
      "1 cls=LSTM name=lstm_1\n",
      "100x512 512x512 512 100x512 512x512 512 100x512 512x512 512 100x512 512x512 512 \n",
      "\n",
      "2 cls=Dropout name=dropout_1\n",
      "\n",
      "\n",
      "3 cls=LSTM name=lstm_2\n",
      "512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 \n",
      "\n",
      "4 cls=Dropout name=dropout_2\n",
      "\n",
      "\n",
      "5 cls=LSTM name=lstm_3\n",
      "512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 \n",
      "\n",
      "6 cls=Dropout name=dropout_3\n",
      "\n",
      "\n",
      "7 cls=SimpleContext name=simplecontext_1\n",
      "\n",
      "\n",
      "8 cls=TimeDistributed name=timedistributed_1\n",
      "944x40000 40000 \n",
      "\n",
      "9 cls=Activation name=activation_1\n",
      "\n",
      "\n",
      "HEAD: patient alleges &apos; dumping &apos; on skid row , sues hospital\n",
      "DESC: a los angeles county woman has filed a lawsuit against an area hospital for alleged &quot; patient dumping . &quot; the lawsuit follows a similar complaint the city of los angeles filed in april against the same hospital and involving the same woman . this newest lawsuit , filed wednesday , alleges that elisa villarreal , 39 , arrived at gardens regional hospital and medical center in hawaiian gardens last september . at that time , the lawsuit says , she was homeless and suffering from auditory^ hallucinations caused by schizophrenia . the hospital staff determined she posed a suicide\n",
      "HEADS:\n",
      "158.86533165 accusations barlow relapse measured wither trans gs inkopolis 14.8 consulate partnernetwork gillette fingerprint rwanda mckee\n",
      "158.896306038 accusations barlow relapse measured wither trans gs inkopolis 14.8 consulate partnernetwork ca-nv landline linemen furthest\n",
      "158.89676857 accusations barlow relapse measured wither trans gs inkopolis 14.8 consulate partnernetwork ca-nv landline linemen ate\n",
      "158.901762962 accusations barlow relapse measured wither trans gs inkopolis 14.8 consulate gearing loxley spec worry carrari\n",
      "158.901968956 accusations barlow relapse measured wither trans gs inkopolis 14.8 consulate gearing loxley spec unloading epiphany\n",
      "158.91458416 accusations barlow relapse measured wither trans gs inkopolis 14.8 consulate gearing loxley spec 2027 legislator\n",
      "158.954888344 wavering major-league cite avenue swinford squad lt. restrain yourselves serena dim contraceptive cabin 1891 2011\n",
      "158.955080986 wavering major-league cite avenue swinford squad lt. restrain yourselves serena dim contraceptive recruiter up-to-the-minute norwalk\n",
      "158.963068008 wavering major-league cite avenue swinford squad lt. restrain yourselves serena dim contraceptive recruiter medecins ---ff\n",
      "158.979554176 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken ales pureplay irfan\n",
      "158.98267746 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken ales oven folders\n",
      "158.983699799 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken straight-line trigger shelters\n",
      "158.984687805 wavering major-league cite avenue swinford squad lt. restrain yourselves serena dim contraceptive recruiter observes liquor\n",
      "158.989592552 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken ales juicy probably\n",
      "158.991641998 eternal regulars all-stars diseased osinbajo attendee angrier consequences blamed intraocular measured kol meters arab alternatively\n",
      "158.997696877 eternal regulars all-stars diseased osinbajo attendee angrier consequences blamed intraocular measured kol breakthrough gimmick 15-20\n",
      "158.999365807 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken ales pureplay orkney\n",
      "159.000558853 eternal regulars all-stars diseased osinbajo attendee angrier consequences blamed subcontinent 15-1 0.17 terra alamy morals\n",
      "159.001948357 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken straight-line trigger quiz\n",
      "159.003599167 eternal regulars all-stars diseased osinbajo attendee angrier consequences replays mikey primaries sunken ales oven retail\n",
      "INFO:tensorflow:Summary name embedding_1_W:0 is illegal; using embedding_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_i:0 is illegal; using lstm_1_W_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_i:0 is illegal; using lstm_1_U_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_i:0 is illegal; using lstm_1_b_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_c:0 is illegal; using lstm_1_W_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_c:0 is illegal; using lstm_1_U_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_c:0 is illegal; using lstm_1_b_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_f:0 is illegal; using lstm_1_W_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_f:0 is illegal; using lstm_1_U_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_f:0 is illegal; using lstm_1_b_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_W_o:0 is illegal; using lstm_1_W_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_U_o:0 is illegal; using lstm_1_U_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1_b_o:0 is illegal; using lstm_1_b_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_W_i:0 is illegal; using lstm_2_W_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_U_i:0 is illegal; using lstm_2_U_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_b_i:0 is illegal; using lstm_2_b_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_W_c:0 is illegal; using lstm_2_W_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_U_c:0 is illegal; using lstm_2_U_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_b_c:0 is illegal; using lstm_2_b_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_W_f:0 is illegal; using lstm_2_W_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_U_f:0 is illegal; using lstm_2_U_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_b_f:0 is illegal; using lstm_2_b_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_W_o:0 is illegal; using lstm_2_W_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_U_o:0 is illegal; using lstm_2_U_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2_b_o:0 is illegal; using lstm_2_b_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_W_i:0 is illegal; using lstm_3_W_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_U_i:0 is illegal; using lstm_3_U_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_b_i:0 is illegal; using lstm_3_b_i_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_W_c:0 is illegal; using lstm_3_W_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_U_c:0 is illegal; using lstm_3_U_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_b_c:0 is illegal; using lstm_3_b_c_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_W_f:0 is illegal; using lstm_3_W_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_U_f:0 is illegal; using lstm_3_U_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_b_f:0 is illegal; using lstm_3_b_f_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_W_o:0 is illegal; using lstm_3_W_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_U_o:0 is illegal; using lstm_3_U_o_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3_b_o:0 is illegal; using lstm_3_b_o_0 instead.\n",
      "INFO:tensorflow:Summary name timedistributed_1_W:0 is illegal; using timedistributed_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name timedistributed_1_b:0 is illegal; using timedistributed_1_b_0 instead.\n",
      "Epoch 1/10\n",
      "640/640 [==============================] - 929s - loss: 10.2809 - val_loss: 9.6484\n",
      "Epoch 2/10\n",
      "640/640 [==============================] - 634s - loss: 8.8067 - val_loss: 8.2044\n",
      "Epoch 3/10\n",
      "640/640 [==============================] - 643s - loss: 8.0560 - val_loss: 8.1940\n",
      "Epoch 4/10\n",
      "640/640 [==============================] - 620s - loss: 8.0188 - val_loss: 8.1371\n",
      "Epoch 5/10\n",
      "640/640 [==============================] - 626s - loss: 7.9900 - val_loss: 8.0919\n",
      "Epoch 6/10\n",
      "640/640 [==============================] - 646s - loss: 7.9785 - val_loss: 8.0740\n",
      "Epoch 7/10\n",
      "640/640 [==============================] - 640s - loss: 7.9086 - val_loss: 8.0729\n",
      "Epoch 8/10\n",
      "640/640 [==============================] - 659s - loss: 7.8320 - val_loss: 8.0626\n",
      "Epoch 9/10\n",
      "640/640 [==============================] - 714s - loss: 7.9538 - val_loss: 8.0553\n",
      "Epoch 10/10\n",
      "640/640 [==============================] - 668s - loss: 7.8749 - val_loss: 8.0507\n",
      "HEAD: reg-db^ x-trackers s &amp; p select frontier ucits etf net asset value ( s )\n",
      "DESC: net asset value ( s ) fund ¦ db x-trackers s &amp; p select frontier ucits etf dealing date ¦ 31-aug-15^ nav per share ¦ usd9.5698^ number of shares in issue ¦ 6,950,965^ code ¦ view source version on businesswire.com : http : / / www.businesswire.com / news / home / 20150902006182^ / en / db x-trackers s &amp; p select frontier ucits etf\n",
      "HEADS:\n",
      "2.57565236092 \n",
      "6.42746925354 :\n",
      "7.29290819168 and\n",
      "7.52711319923 a\n",
      "10.2792818546 : :\n",
      "10.3399159908 to :\n",
      "12.7164764404 &apos;s &apos;\n",
      "14.3370883465 : : ,\n",
      "15.1390662193 6,950,965^ , of\n",
      "15.919216156 6,950,965^ to s\n",
      "16.2784523964 to : as\n",
      "18.2495260239 : : , to\n",
      "23.5929648876 6,950,965^ , of , of\n",
      "26.2370266914 : : , to the :\n",
      "28.0882263184 6,950,965^ , of , of in\n",
      "30.3727066517 : : , to the : the\n",
      "37.5752651691 6,950,965^ , of , of in pa\n",
      "50.1842730045 : : , to the : the all to to in\n",
      "64.5720798969 : : , to the : the all to to / , : : :\n",
      "67.4276025295 6,950,965^ , of , of in face gun with better renegade\n",
      "Time passed is 6974.895951271057\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "activation_rnn_size = 40 if maxlend else 0\n",
    "\n",
    "# training parameters\n",
    "seed = 42\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "optimizer = 'adam'\n",
    "LR = args.lr\n",
    "batch_size = args.batch_size\n",
    "\n",
    "nb_train_samples = np.int(np.floor(args.nsamples / batch_size)) * batch_size\n",
    "nb_val_samples = nb_train_samples\n",
    "\n",
    "\n",
    "# read word embedding\n",
    "with open(os.path.join(fconfig.path_data, '{}.pkl'.format(args.FN0)), 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape\n",
    "\n",
    "with open(os.path.join(fconfig.path_data, '{}.data.pkl'.format(args.FN0)), 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)\n",
    "\n",
    "nb_unknown_words = 10\n",
    "\n",
    "print('number of examples', len(X), len(Y))\n",
    "print('dimension of embedding space for words: {:,}'.format(embedding_size))\n",
    "print('vocabulary size: {:,} the last {:,} words can be used as place holders for unknown/oov words'.\n",
    "      format(vocab_size, nb_unknown_words))\n",
    "print('total number of different words: {:,}'.format(len(idx2word)))\n",
    "print('number of words outside vocabulary which we can substitue using glove similarity: {:,}'.\n",
    "      format(len(glove_idx2idx)))\n",
    "print('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov): {:,}'.\n",
    "      format(len(idx2word) - vocab_size - len(glove_idx2idx)))\n",
    "print()\n",
    "\n",
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size - 1 - i] = '<{}>'.format(i)\n",
    "\n",
    "# when printing mark words outside vocabulary with `^` at their end\n",
    "oov0 = vocab_size - nb_unknown_words\n",
    "\n",
    "for i in range(oov0, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i] + '^'\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nb_val_samples, random_state=seed)\n",
    "len(X_train), len(Y_train), len(X_test), len(Y_test)\n",
    "del X\n",
    "del Y\n",
    "\n",
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'\n",
    "\n",
    "def prt(label, x):\n",
    "    print(label + ':',)\n",
    "    print_str = ''\n",
    "    for w in x:\n",
    "        print_str += idx2word[w] + ' '\n",
    "    print(print_str)\n",
    "    print()\n",
    "\n",
    "print('Random head, description:')\n",
    "i = 811\n",
    "prt('H', Y_train[i])\n",
    "prt('D', X_train[i])\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "# seed weight initialization\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "regularizer = l2(weight_decay) if weight_decay else None\n",
    "\n",
    "\n",
    "# start with a standaed stacked LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size,\n",
    "                    input_length=maxlen,\n",
    "                    W_regularizer=regularizer, dropout=p_emb, weights=[embedding], mask_zero=True,\n",
    "                    name='embedding_1'))\n",
    "for i in range(rnn_layers):\n",
    "    lstm = LSTM(rnn_size, return_sequences=True,\n",
    "                W_regularizer=regularizer, U_regularizer=regularizer,\n",
    "                b_regularizer=regularizer, dropout_W=p_W, dropout_U=p_U,\n",
    "                name='lstm_{}'.format(i + 1))\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(p_dense, name='dropout_{}'.format(i + 1)))\n",
    "\n",
    "\n",
    "# A special layer that reduces the input just to its headline part (second half).\n",
    "# For each word in this part it concatenate the output of the previous layer (RNN)\n",
    "# with a weighted average of the outputs of the description part.\n",
    "# In this only the last `rnn_size - activation_rnn_size` are used from each output.\n",
    "# The first `activation_rnn_size` output is used to computer the weights for the averaging.\n",
    "\n",
    "def simple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:, :maxlend, :], X[:, maxlend:, :]\n",
    "    head_activations, head_words = head[:, :, :n], head[:, :, n:]\n",
    "    desc_activations, desc_words = desc[:, :, :n], desc[:, :, n:]\n",
    "\n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2, 2))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20 * K.expand_dims(\n",
    "        1. - K.cast(mask[:, :maxlend], 'float32'), 1)\n",
    "\n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies, (-1, maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights, (-1, maxlenh, maxlend))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2, 1))\n",
    "    return K.concatenate((desc_avg_word, head_words))\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SimpleContext, self).__init__(simple_context, **kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2 * (rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)\n",
    "\n",
    "if activation_rnn_size:\n",
    "    model.add(SimpleContext(name='simplecontext_1'))\n",
    "\n",
    "model.add(TimeDistributed(Dense(vocab_size,\n",
    "                                W_regularizer=regularizer, b_regularizer=regularizer,\n",
    "                                name='timedistributed_1')))\n",
    "model.add(Activation('softmax', name='activation_1'))\n",
    "\n",
    "# opt = Adam(lr=LR)  # keep calm and reduce learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "K.set_value(model.optimizer.lr, np.float32(LR))\n",
    "\n",
    "def str_shape(x):\n",
    "    return 'x'.join(list(map(str, x.shape)))\n",
    "\n",
    "def inspect_model(model):\n",
    "    for i, l in enumerate(model.layers):\n",
    "        print(i, 'cls={} name={}'.format(type(l).__name__, l.name))\n",
    "        weights = l.get_weights()\n",
    "        print_str = ''\n",
    "        for weight in weights:\n",
    "            print_str += str_shape(weight) + ' '\n",
    "        print(print_str)\n",
    "        print()\n",
    "\n",
    "inspect_model(model)\n",
    "\n",
    "# Load\n",
    "FN1_filename = os.path.join(fconfig.path_models, '{}.hdf5'.format(args.FN1))\n",
    "if args.FN1 and os.path.exists(FN1_filename):\n",
    "    model.load_weights(FN1_filename)\n",
    "    print('Model weights loaded from {}'.format(FN1_filename))\n",
    "\n",
    "# Test\n",
    "def lpadd(x, maxlend=maxlend, eos=eos):\n",
    "    \"\"\"left (pre) pad a description to maxlend and then add eos.\n",
    "    The eos is the input to predicting the first word in the headline\n",
    "    \"\"\"\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty] * (maxlend - n) + x + [eos]\n",
    "\n",
    "samples = [lpadd([3] * 26)]\n",
    "# pad from right (post) so the first maxlend will be description followed by headline\n",
    "data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "\n",
    "np.all(data[:, maxlend] == eos)\n",
    "data.shape, list(map(len, samples))\n",
    "probs = model.predict(data, verbose=0, batch_size=1)\n",
    "probs.shape\n",
    "\n",
    "# Sample generation\n",
    "\n",
    "# this section is only used to generate examples. you can skip it if you just want to understand how the training works\n",
    "\n",
    "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
    "def beamsearch(predict, start=[empty] * maxlend + [eos],\n",
    "               k=1, maxsample=maxlen, use_unk=True, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n elements according to their energy\"\"\"\n",
    "        n = min(n, len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature)\n",
    "        res = []\n",
    "        for i in range(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb / z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0.  # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1  # samples that did not yet reached eos\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k:\n",
    "        # for every possible live sample calc prob for every possible label\n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:, None] - np.log(probs)\n",
    "        cand_scores[:, empty] = 1e20\n",
    "        if not use_unk:\n",
    "            for i in range(nb_unknown_words):\n",
    "                cand_scores[:, vocab_size - 1 - i] = 1e20\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        ranks_dead = [r for r in ranks if r < n]\n",
    "        ranks_live = [r - n for r in ranks if r >= n]\n",
    "\n",
    "        dead_scores = [dead_scores[r] for r in ranks_dead]\n",
    "        dead_samples = [dead_samples[r] for r in ranks_dead]\n",
    "\n",
    "        live_scores = [live_scores[r] for r in ranks_live]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r // voc_size] + [r % voc_size] for r in ranks_live]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        zombie = [s[-1] == eos or len(s) > maxsample for s in live_samples]\n",
    "\n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s, z in zip(live_samples, zombie) if z]\n",
    "        dead_scores += [s for s, z in zip(live_scores, zombie) if z]\n",
    "        # remove zombies from the living\n",
    "        live_samples = [s for s, z in zip(live_samples, zombie) if not z]\n",
    "        live_scores = [s for s, z in zip(live_scores, zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return dead_samples + live_samples, dead_scores + live_scores\n",
    "\n",
    "def keras_rnn_predict(samples, empty=empty, model=model, maxlen=maxlen):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    sample_lengths = list(map(len, samples))\n",
    "    assert all(l > maxlend for l in sample_lengths)\n",
    "    assert all(l[maxlend] == eos for l in samples)\n",
    "    # pad from right (post) so the first maxlend will be description followed by headline\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    probs = model.predict(data, verbose=0, batch_size=batch_size)\n",
    "    return np.array([prob[sample_length - maxlend - 1]\n",
    "                     for prob, sample_length in zip(probs, sample_lengths)])\n",
    "\n",
    "def vocab_fold(xs):\n",
    "    \"\"\"convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
    "    \"\"\"\n",
    "    xs = [x if x < oov0 else glove_idx2idx.get(x, x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= oov0])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
    "    outside = dict((x, vocab_size - 1 - min(i, nb_unknown_words - 1)) for i, x in enumerate(outside))\n",
    "    xs = [outside.get(x, x) for x in xs]\n",
    "    return xs\n",
    "\n",
    "def vocab_unfold(desc, xs):\n",
    "    # assume desc is the unfolded version of the start of xs\n",
    "    unfold = {}\n",
    "    for i, unfold_idx in enumerate(desc):\n",
    "        fold_idx = xs[i]\n",
    "        if fold_idx >= oov0:\n",
    "            unfold[fold_idx] = unfold_idx\n",
    "    return [unfold.get(x, x) for x in xs]\n",
    "\n",
    "def gensamples(skips=2, k=10, batch_size=batch_size, short=False, temperature=1., use_unk=True):\n",
    "    i = random.randint(0, len(X_test) - 1)\n",
    "    print('HEAD:', ' '.join(idx2word[w] for w in Y_test[i][:maxlenh]))\n",
    "    print('DESC:', ' '.join(idx2word[w] for w in X_test[i][:maxlend]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print('HEADS:')\n",
    "    x = X_test[i]\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend, len(x)), max(maxlend, len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s, start, scr) for s, scr in zip(sample, score)]\n",
    "\n",
    "    samples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w // (256 * 256)) + chr((w // 256) % 256) + chr(w % 256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code, c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print(score, ' '.join(words))\n",
    "        else:\n",
    "                print(score, ' '.join(words))\n",
    "        codes.append(code)\n",
    "\n",
    "gensamples(skips=2, batch_size=batch_size, k=10, temperature=args.temperature)\n",
    "\n",
    "\n",
    "# Data generator\n",
    "\n",
    "\"\"\"Data generator generates batches of inputs and outputs/labels for training. The inputs are each made from two parts. The first maxlend words are the original description, followed by `eos` followed by the headline which we want to predict, except for the last word in the headline which is always `eos` and then `empty` padding until `maxlen` words.\n",
    "For each, input, the output is the headline words (without the start `eos` but with the ending `eos`) padded with `empty` words up to `maxlenh` words. The output is also expanded to be y-hot encoding of each word.\n",
    "To be more realistic, the second part of the input should be the result of generation and not the original headline.\n",
    "Instead we will flip just `nflips` words to be from the generator, but even this is too hard and instead\n",
    "implement flipping in a naive way (which consumes less time.) Using the full input (description + eos + headline) generate predictions for outputs. For nflips random words from the output, replace the original word with the word with highest probability from the prediction.\n",
    "\"\"\"\n",
    "\n",
    "def flip_headline(x, nflips=None, model=None, debug=False):\n",
    "    \"\"\"given a vectorized input (after `pad_sequences`) flip some of the words in the second half (headline)\n",
    "    with words predicted by the model\n",
    "    \"\"\"\n",
    "    if nflips is None or model is None or nflips <= 0:\n",
    "        return x\n",
    "\n",
    "    batch_size = len(x)\n",
    "    assert np.all(x[:, maxlend] == eos)\n",
    "    probs = model.predict(x, verbose=0, batch_size=batch_size)\n",
    "    x_out = x.copy()\n",
    "    for b in range(batch_size):\n",
    "        # pick locations we want to flip\n",
    "        # 0...maxlend-1 are descriptions and should be fixed\n",
    "        # maxlend is eos and should be fixed\n",
    "        flips = sorted(random.sample(range(maxlend + 1, maxlen), nflips))\n",
    "        if debug and b < debug:\n",
    "            print(b)\n",
    "        for input_idx in flips:\n",
    "            if x[b, input_idx] == empty or x[b, input_idx] == eos:\n",
    "                continue\n",
    "            # convert from input location to label location\n",
    "            # the output at maxlend (when input is eos) is feed as input at maxlend+1\n",
    "            label_idx = input_idx - (maxlend + 1)\n",
    "            prob = probs[b, label_idx]\n",
    "            w = prob.argmax()\n",
    "            if w == empty:  # replace accidental empty with oov\n",
    "                w = oov0\n",
    "            if debug and b < debug:\n",
    "                print('{} => {}'.format(idx2word[x_out[b, input_idx]], idx2word[w]),)\n",
    "            x_out[b, input_idx] = w\n",
    "        if debug and b < debug:\n",
    "            print()\n",
    "    return x_out\n",
    "\n",
    "def conv_seq_labels(xds, xhs, nflips=None, model=None, debug=False):\n",
    "    \"\"\"description and hedlines are converted to padded input vectors. headlines are one-hot to label\"\"\"\n",
    "    batch_size = len(xhs)\n",
    "    assert len(xds) == batch_size\n",
    "    x = [vocab_fold(lpadd(xd) + xh) for xd, xh in zip(xds, xhs)]  # the input does not have 2nd eos\n",
    "    x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    x = flip_headline(x, nflips=nflips, model=model, debug=debug)\n",
    "\n",
    "    y = np.zeros((batch_size, maxlenh, vocab_size))\n",
    "    for i, xh in enumerate(xhs):\n",
    "        xh = vocab_fold(xh) + [eos] + [empty] * maxlenh  # output does have a eos at end\n",
    "        xh = xh[:maxlenh]\n",
    "        y[i, :, :] = np_utils.to_categorical(xh, vocab_size)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def gen(Xd, Xh, batch_size=batch_size, nb_batches=None, nflips=None, model=None, debug=False, seed=seed):\n",
    "    \"\"\"yield batches. for training use nb_batches=None\n",
    "    for validation generate deterministic results repeating every nb_batches\n",
    "    while training it is good idea to flip once in a while the values of the headlines from the\n",
    "    value taken from Xh to value generated by the model.\n",
    "    \"\"\"\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        xds = []\n",
    "        xhs = []\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, 2e10)\n",
    "        random.seed(c + 123456789 + seed)\n",
    "        for b in range(batch_size):\n",
    "            t = random.randint(0, len(Xd) - 1)\n",
    "\n",
    "            xd = Xd[t]\n",
    "            s = random.randint(min(maxlend, len(xd)), max(maxlend, len(xd)))\n",
    "            xds.append(xd[:s])\n",
    "\n",
    "            xh = Xh[t]\n",
    "            s = random.randint(min(maxlenh, len(xh)), max(maxlenh, len(xh)))\n",
    "            xhs.append(xh[:s])\n",
    "\n",
    "        # undo the seeding before we yield inorder not to affect the caller\n",
    "        c += 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(xds, xhs, nflips=nflips, model=model, debug=debug)\n",
    "\n",
    "def test_gen(gen, n=5):\n",
    "    Xtr, Ytr = next(gen)\n",
    "    for i in range(n):\n",
    "        assert Xtr[i, maxlend] == eos\n",
    "        x = Xtr[i, :maxlend]\n",
    "        y = Xtr[i, maxlend:]\n",
    "        yy = Ytr[i, :]\n",
    "        yy = np.where(yy)[1]\n",
    "        prt('L', yy)\n",
    "        prt('H', y)\n",
    "        if maxlend:\n",
    "            prt('D', x)\n",
    "\n",
    "r = next(gen(X_train, Y_train, batch_size=batch_size))\n",
    "valgen = gen(X_test, Y_test, nb_batches=3, batch_size=batch_size)\n",
    "\n",
    "# Train\n",
    "history = {}\n",
    "traingen = gen(X_train, Y_train, batch_size=batch_size, nflips=args.nflips, model=model)\n",
    "valgen = gen(X_test, Y_test, nb_batches=nb_val_samples // batch_size, batch_size=batch_size)\n",
    "\n",
    "callbacks = [TensorBoard(\n",
    "    log_dir=os.path.join(fconfig.path_logs, str(time.time())),\n",
    "    histogram_freq=2, write_graph=False, write_images=False)]\n",
    "\n",
    "h = model.fit_generator(\n",
    "    traingen, samples_per_epoch=nb_train_samples,\n",
    "    nb_epoch=args.epochs, validation_data=valgen, nb_val_samples=nb_val_samples,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "for k, v in h.history.items():\n",
    "    history[k] = history.get(k, []) + v\n",
    "with open(os.path.join(fconfig.path_models, 'history.pkl'.format(\n",
    ")), 'wb') as fp:\n",
    "    pickle.dump(history, fp, -1)\n",
    "model.save_weights(FN1_filename, overwrite=True)\n",
    "gensamples(skips=2, batch_size=batch_size, k=10, temperature=args.temperature)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time passed is %s\"%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD: man dies after car collides^ with tree and lamppost^ in dudley\n",
      "DESC: a man has died after the car he was driving collided with a tree and lamppost^ in dudley last night ( september 29 ) . west midlands ambulance service was called to reports of a single car which had left the road at the junction of the birmingham new road and priory^ road at 10.20pm^ yesterday . a paramedic area support officer and two ambulance crews attended the scene , but despite their best efforts the man went into cardiac arrest and was confirmed dead a short time later . a west midlands ambulance service spokeswoman said : “ when\n",
      "HEADS:\n",
      "2.57538557053 \n",
      "2.57542085648 \n",
      "6.42714238167 :\n",
      "7.04125666618 for\n",
      "11.4461870193 : &apos;\n",
      "11.8024477959 , on\n",
      "11.865852356 the with\n",
      "13.7911877632 the red\n",
      "15.0560615063 : and to\n",
      "15.7778766155 the with to\n",
      "16.0015654564 the with the\n",
      "23.4199593067 : and , 10.20pm^ 10.20pm^\n",
      "48.0664536953 : and , 10.20pm^ 10.20pm^ % ’ to 10.20pm^ of\n",
      "48.8467357159 the with to of to to to to to and of\n",
      "49.8110017776 the with to of to to to to to and (\n",
      "51.9784708023 : and , 10.20pm^ 10.20pm^ % ’ to 10.20pm^ of to\n",
      "56.0362577438 : and , 10.20pm^ 10.20pm^ % ’ to 10.20pm^ of to ,\n",
      "57.2728931904 the with to of to to to to to and of officials\n",
      "69.1330032349 the with to of to to to to to and ( london <0>^ and <0>^\n",
      "76.3964102268 : and , 10.20pm^ 10.20pm^ % ’ to 10.20pm^ of to , popular ) students\n"
     ]
    }
   ],
   "source": [
    "gensamples(skips=2, batch_size=batch_size, k=10, temperature=args.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this part is taken from the predict.ipynb due to issues i had in rebuilding the model and loading the weights\n",
    "\n",
    "with open(os.path.join('data/', 'vocabulary-embedding.data.pkl'), 'rb') as fp:\n",
    "    X_data, Y_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.s . , china make progress ; differences lurk \n",
      "president barack obama said friday that china and the united states had agreed to work to avoid military misunderstandings^ and desist from cybertheft^ for commercial gain during a state visit with chinese president xi jinping . first lady michelle obama and china &apos;s first lady peng liyuan^ announced friday that the panda born last month in washington , d.c . , has been named &quot; bei bei , &quot; which means &quot; precious treasure , &quot; according to the smithsonian &apos;s national zoo . chinese president xi jinping has faced a barrage of criticism from u.s. presidential hopefuls recently , but american and other foreign students living in china think he &apos;s a handsome , wise and humble leader , who &apos;s strong on foreign policy . at least th ... during his second day in seattle , chinese president xi jinping met wednesday with some of the biggest names in tech and then toured a boeing airplane factory . the aerospace and defense giant has a history of close ties to china . boeing helped develop ... chinese president xi jinping arrives in washington on thursday for his first official state visit , and his interactions with president barack obama are sure to include some awkward conversations . xi will be welcomed to the white house at two dinners - ... a key measure of china &apos;s all-important manufacturing sector has nose-dived^ to its lowest level in 78 months , yet another sign that the country &apos;s factories are running out of steam . the &quot; flash &quot; measure of sentiment among manufacturing purchasing manag^ ... president xi jinping said that china is ready to open a &quot; high-level &quot; dialogue with the u.s. to fight cybercrime^ , while insisting that his government has never engaged in hacking . &quot; commercial cybertheft^ and hacking against government networks are crime ... but his guest , appearing beside him at a news conference in the white house rose garden , staunchly defended china &apos;s territorial claims , which are inflaming^ tensions in asia and testing china &apos;s ties with washington , a security guarantor^ in the region since world war ii . the summit , which opened with a 21-gun^ salute and later friday will feature a state dinner , comes at a time when the two great powers are seeking to find areas of cooperation on issues like climate and terrorism . but the effort comes amidst rising tensions on subjects including territorial claims and cyberhacking^ that have fueled fears that the two countries are destined for a new era of confrontation . &quot; even as our nations cooperate , i believe , and i know you agree , that we must address our differences candidly^ , &quot; obama told xi . the chinese leader , speaking through a translator , also admitted that the nations do not see eye-to-eye on everything and told obama they needed to &quot; respect each other &apos;s interests and concerns , ( and ) be broadminded^ about our differences and disagreements . &quot; those differences were on display as obama announced a deal under which the two sides agreed not to conduct cybertheft^ against one another but warned that he was still ready to impose sanctions against chinese entities proven to have taken part in such activity -- a threat that overshadowed the runup^ to the summit . &quot; i indicated that it has to stop , &quot; obama said . &quot; what i &apos;ve said to president xi and what i say to the american people is , the question now is , are words followed by actions ? and we will be watching carefully to make an assessment as to whether progress has been made in this area . &quot; the announcement comes on the heels of the massive office of personnel management hack announced in june -- which officials have blamed on the chinese . on wednesday , opm^ said that up to 5.6 million fingerprints were among the more than 21.5 million current , former and prospective federal employee records stolen in the hack . there was no undertaking by the two countries not to conduct cyber espionage by intelligence agencies , however . obama and xi also discussed rising regional tensions in asia sparked by beijing &apos;s construction of military installations on man-made islands and disputed reefs^ in the south china sea . &quot; we agreed to new channels of communication to reduce the risk of miscalculations between our militaries^ , &quot; obama said . the deals require captains of naval vessels to ensure prompt communication , to make their intentions clear , to maintain a safe distance and to avoid &quot; uncivil^ language &quot; or &quot; unfriendly physical gestures &quot; to head off collisions that could mushroom into national security standoffs^ . the agreement also says that aircraft flying in international airspace have the right to defend themselves but should respect the rights of the other side as well . in addition , the united states and china also reached an agreement to facilitate crisis communications between the u.s. and chinese militaries^ meant to defuse tensions and to avoid escalations^ after any incidents between their armed forces . the moves come amid rising tension in the south and east china seas over china &apos;s territorial claims and after u.s. officials this week accused a chinese jet pilot of making a unsafe pass near a u.s. aircraft over the yellow sea . the intercept follows a more dangerous maneuver last year when an armed chinese fighter jet came within approximately 20 feet of a u.s . navy p-8^ aircraft , at one point rolling to its side to show the u.s. plane its weapons load , pentagon officials said at the time . beijing has taken an increasingly aggressive posture in the south china sea and disputes over territories in the east china sea continue to cause tension between beijing and its neighbors , many of which are close u.s . allies . in the south china sea , china is building a series of man-made , militarized islands 600 miles off its coastline and then claiming the surrounding air and sea rights . in the east china sea , china has claimed sovereignty over islands -- some uninhabited^ -- that japan also claims . but xi made clear that despite u.s. concerns and rising anger among washington &apos;s allies in the region , china has no intention of ceding ground on its territorial claims . &quot; islands in the south china sea since ancient times are china &apos;s territory . we have the right to uphold our own territorial sovereignty and lawful and legitimate maritime rights and interests , &quot; xi said . he said that china was entitled to build structures on disputed atolls^ in the spratly^ islands in the south china sea but did not intend to militarize^ them . in may , a u.s. surveillance plane carrying a cnn crew was warned eight times by the chinese navy after it flew over chinese installations in the area . the united states does not take a position on the sovereignty of disputed islands in the area but wants the issue resolved multilaterally^ , officials have said . china , seeking to exploit its size and influence , prefers the disputes to be resolved one-on-one with nations involved and rejects a role four what it sees as outsiders , like the united states . obama , who has been criticized , including by republican 2016 presidential candidates , of doing too little to pressure china over its human rights record , pointedly^ brought up the issue in front of xi at the news conference . &quot; we recognize that there are real differences there , and president xi shared his views in terms of how he can move forward in a step-by-step way that preserves chinese unity , &quot; he said . obama also mentioned the name of tibet &apos;s spiritual leader -- who is regarded by china as a separatist . &quot; even as we recognize tibet is part of the people &apos;s republic of china , we continue to encourage chinese authorities to preserve the religious and cultural identity of the tibetan people , and to engage the dalai lama or his representatives , &quot; he said . xi said he was willing to have a human rights dialogue with the united states , but as is customary with chinese leaders , he pointed out that the concept of human rights was seen differently in beijing &quot; we must recognize that countries have different historical processes and realities , that we need to respect people of all countries in the rights to choose their own development independently , &quot; he said . and with the eyes of the world on china &apos;s economy , amid fears that a slowdown and a stock market clump^ could tip the world into recession , xi promised a &quot; proactive fiscal policy and prudent monetary policy &quot; to restore growth rates to around 7 % . \n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(X_data))\n",
    "sample_str = ''\n",
    "sample_title = ''\n",
    "for w in X_data[i]:\n",
    "    sample_str += idx2word[w] + ' '\n",
    "for w in Y_data[i]:\n",
    "    sample_title += idx2word[w] + ' '\n",
    "print(sample_title)\n",
    "print(sample_str)\n",
    "sample_title = Y_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_gensamples(X=None, X_test=None, Y_test=None, avoid=None, avoid_score=1, skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
    "    if X is None or isinstance(X,int):\n",
    "        if X is None:\n",
    "            i = random.randint(0,len(X_test)-1)\n",
    "        else:\n",
    "            i = X\n",
    "        print('HEAD %d:'%i,' '.join(idx2word[w] for w in Y_test[i]))\n",
    "        print('DESC:',' '.join(idx2word[w] for w in X_test[i]))\n",
    "        sys.stdout.flush()\n",
    "        x = X_test[i]\n",
    "    else:\n",
    "        x = [word2idx[w.rstrip('^')] for w in X.split()]\n",
    "        \n",
    "    if avoid:\n",
    "        # avoid is a list of avoids. Each avoid is a string or list of word indeicies\n",
    "        if isinstance(avoid,str) or isinstance(avoid[0], int):\n",
    "            avoid  [avoid]\n",
    "        avoid = [a.split() if isinstance(a,str) else a for a in avoid]\n",
    "        avoid = [[a] for a in avoid]\n",
    "\n",
    "    print('HEADS:')\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    print(skips)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = pred_beamsearch(predict=keras_rnn_predict, start=fold_start, avoid=avoid, avoid_score=avoid_score,\n",
    "                                   k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
    "\n",
    "    s' '.join(idx2word[w] for w in headline)amples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print(score, ' '.join(words))\n",
    "        else:\n",
    "                print(score, ' '.join(words))\n",
    "        codes.append(code)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_beamsearch(predict, start=[empty]*maxlend + [eos], avoid=None, avoid_score=1,\n",
    "               k=1, maxsample=maxlen, use_unk=True, oov=vocab_size-1, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n different elements according to their energy\"\"\"\n",
    "        n = min(n,len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature )\n",
    "        res = []\n",
    "        for i in range(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0. # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_samples:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "        assert vocab_size == probs.shape[1]\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        cand_scores[:,empty] = 1e20\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        if avoid:\n",
    "            for a in avoid:\n",
    "                for i, s in enumerate(live_samples):\n",
    "                    n = len(s) - len(start)\n",
    "                    if n < len(a):\n",
    "                        # at this point live_sample is before the new word,\n",
    "                        # which should be avoided, is added\n",
    "                        cand_scores[i,a[n]] += avoid_score\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "        \n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        dead_scores = [dead_scores[r] for r in ranks if r < n]\n",
    "        dead_samples = [dead_samples[r] for r in ranks if r < n]\n",
    "        \n",
    "        live_scores = [live_scores[r-n] for r in ranks if r >= n]\n",
    "        live_samples = [live_samples[(r-n)//vocab_size]+[(r-n)%vocab_size] for r in ranks if r >= n]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        def is_zombie(s):\n",
    "            return s[-1] == eos or len(s) > maxsample\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_scores += [c for s, c in zip(live_samples, live_scores) if is_zombie(s)]\n",
    "        dead_samples += [s for s in live_samples if is_zombie(s)]\n",
    "        \n",
    "        # remove zombies from the living \n",
    "        live_scores = [c for s, c in zip(live_samples, live_scores) if not is_zombie(s)]\n",
    "        live_samples = [s for s in live_samples if not is_zombie(s)]\n",
    "\n",
    "    return dead_samples, dead_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "range(100, 1517, 709)\n",
      "2.57550764084 \n",
      "14.8342759609 to in :\n"
     ]
    }
   ],
   "source": [
    "samples = pred_gensamples(sample_str, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headline = samples[0][0][len(samples[0][1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(idx2word[w] for w in headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avoid = headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "range(100, 1517, 709)\n",
      "2.67538437843 \n",
      "6.48762273788 to\n"
     ]
    }
   ],
   "source": [
    "samples = pred_gensamples(sample_str, avoid=avoid, avoid_score=.1, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avoid = samples[0][0][len(samples[0][1]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADS:\n",
      "range(100, 1517, 709)\n",
      "2.67538437843 \n",
      "13.3829202652 it cybertheft^\n",
      "19.4339079857 : : &apos; the\n"
     ]
    }
   ],
   "source": [
    "samples = pred_gensamples(sample_str, avoid=avoid, avoid_score=.1, skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=black>WEIGHTS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wsimple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:,:maxlend], X[:,maxlend:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=([2],[2]))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    #assert mask.get_shape().ndim == 2\n",
    "    #activation_energies = K.switch(mask[:, None, :maxlend], activation_energies, -1e20)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    return activation_weights\n",
    "\n",
    "\n",
    "class WSimpleContext(Lambda):\n",
    "    def __init__(self):\n",
    "        super(WSimpleContext, self).__init__(wsimple_context)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2*(rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wmodel = Sequential()\n",
    "wmodel.add(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wmodel.add(SimpleContext())\n",
    "#inspect_model(model)\n",
    "tmodel = Sequential()\n",
    "#inspect_model(tmodel)\n",
    "tmodel.add(model.get_layer('embedding_1'))\n",
    "tmodel.add(model.get_layer('lstm_1'))\n",
    "tmodel.add(model.get_layer('dropout_1'))\n",
    "tmodel.add(model.get_layer('lstm_2'))\n",
    "tmodel.add(model.get_layer('dropout_2'))\n",
    "tmodel.add(model.get_layer('lstm_3'))\n",
    "tmodel.add(model.get_layer('dropout_3'))\n",
    "#tmodel.add(model.get_layer('simplecontext_1'))\n",
    "#wmodel.add(WSimpleContext())\n",
    "#wmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#inspect_model(tmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmodel.add(model.get_layer('simplecontext_1'))\n",
    "tmodel.add(WSimpleContext())\n",
    "#tmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls=Embedding name=embedding_1\n",
      "40000x100 \n",
      "\n",
      "1 cls=LSTM name=lstm_1\n",
      "100x512 512x512 512 100x512 512x512 512 100x512 512x512 512 100x512 512x512 512 \n",
      "\n",
      "2 cls=Dropout name=dropout_1\n",
      "\n",
      "\n",
      "3 cls=LSTM name=lstm_2\n",
      "512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 \n",
      "\n",
      "4 cls=Dropout name=dropout_2\n",
      "\n",
      "\n",
      "5 cls=LSTM name=lstm_3\n",
      "512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 512x512 512x512 512 \n",
      "\n",
      "6 cls=Dropout name=dropout_3\n",
      "\n",
      "\n",
      "7 cls=WSimpleContext name=wsimplecontext_27\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_model(tmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=black>Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 8\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_weight = K.variable(1.)\n",
    "head_weight = K.variable(1.)\n",
    "cross_weight = K.variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#because it runs on tensorflow backend, the set_value function is under backend and not under variable\n",
    "K.set_value(context_weight,np.float32(1.))\n",
    "K.set_value(head_weight,np.float32(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = samples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'president barack obama said friday that china and the united states had agreed to work to avoid military misunderstanding and desist from <0>^ for commercial gain during a state visit with chinese president xi jinping . first lady michelle obama and china &apos;s first lady peng kalra announced friday that the panda born last month in washington , d.c . , has been named &quot; bei bei , &quot; which means &quot; precious treasure , &quot; according to the smithsonian &apos;s national zoo . chinese president xi jinping has faced a barrage of criticism from u.s. presidential hopefuls recently , ~ ~'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[w] for w in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 115)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sequence.pad_sequences([sample], maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  187  3635  1005    38   266    14   350     5     2   254   279    68\n",
      "  1906     6   153     6  1461  1065 24033     5 27551    31 39999    10\n",
      "   873  1762   162     8   129   181    16  1141   187  2869  6903     4\n",
      "    73  2742  4327  1005     5   350    23    73  2742 28625 38491   352\n",
      "   266    14     2 14169  1767   105   351     9   642     3  5381     4\n",
      "     3    37    63  1066    12 17986 17986     3    12    60   715    12\n",
      "  3863  7909     3    12   240     6     2 29780    23   211  4952     4\n",
      "  1141   187  2869  6903    37  2834     8 24455     7  3473    31   579\n",
      "  1347 15086   673     3     1     1     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 115, 512)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel.predict(data, verbose=0, batch_size=1)\n",
    "tweights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "startd = np.where(data[0,:] != empty)[0][0]\n",
    "lenh = np.where(data[0,maxlend+1:] == eos)[0][0]\n",
    "print(startd, lenh)\n",
    "print(data[0,maxlend+1:])\n",
    "print(np.where(data[0,maxlend+1:] == eos)[0])\n",
    "\n",
    "lenh=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb0kq6H8A2uGyUwqU2W0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12097d18e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.hist(np.array(weights[0,:lenh,startd:].flatten()+1), bins=100);\n",
    "plt.hist(np.array(tweights[0,:lenh,startd:].flatten()+1), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def heat(sample,weights,dark=0.3):\n",
    "    weights = (weights - weights.min())/(weights.max() - weights.min() + 1e-4)\n",
    "    html = ''\n",
    "    fmt = ' <span style=\"background-color: #{0:x}{0:x}ff\">{1}</span>'\n",
    "    for t,w in zip(sample,weights):\n",
    "        c = int(256*((1.-dark)*(1.-w)+dark))\n",
    "        html += fmt.format(c,idx2word[t])\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #ffffff\">president</span> <span style=\"background-color: #4d4dff\">barack</span> <span style=\"background-color: #dadaff\">obama</span> <span style=\"background-color: #d7d7ff\">said</span> <span style=\"background-color: #f6f6ff\">friday</span> <span style=\"background-color: #eaeaff\">that</span> <span style=\"background-color: #d0d0ff\">china</span> <span style=\"background-color: #e3e3ff\">and</span> <span style=\"background-color: #efefff\">the</span> <span style=\"background-color: #e5e5ff\">united</span> <span style=\"background-color: #e4e4ff\">states</span> <span style=\"background-color: #f4f4ff\">had</span> <span style=\"background-color: #fafaff\">agreed</span> <span style=\"background-color: #f2f2ff\">to</span> <span style=\"background-color: #fefeff\">work</span> <span style=\"background-color: #ceceff\">to</span> <span style=\"background-color: #f2f2ff\">avoid</span> <span style=\"background-color: #f1f1ff\">military</span> <span style=\"background-color: #fdfdff\">misunderstanding</span> <span style=\"background-color: #f9f9ff\">and</span> <span style=\"background-color: #f4f4ff\">desist</span> <span style=\"background-color: #f9f9ff\">from</span> <span style=\"background-color: #f6f6ff\"><0>^</span> <span style=\"background-color: #f1f1ff\">for</span> <span style=\"background-color: #f4f4ff\">commercial</span> <span style=\"background-color: #ffffff\">gain</span> <span style=\"background-color: #f8f8ff\">during</span> <span style=\"background-color: #fefeff\">a</span> <span style=\"background-color: #fcfcff\">state</span> <span style=\"background-color: #fdfdff\">visit</span> <span style=\"background-color: #fefeff\">with</span> <span style=\"background-color: #f6f6ff\">chinese</span> <span style=\"background-color: #f4f4ff\">president</span> <span style=\"background-color: #ffffff\">xi</span> <span style=\"background-color: #fdfdff\">jinping</span> <span style=\"background-color: #fefeff\">.</span> <span style=\"background-color: #fcfcff\">first</span> <span style=\"background-color: #fdfdff\">lady</span> <span style=\"background-color: #ffffff\">michelle</span> <span style=\"background-color: #fefeff\">obama</span> <span style=\"background-color: #fdfdff\">and</span> <span style=\"background-color: #fefeff\">china</span> <span style=\"background-color: #fdfdff\">&apos;s</span> <span style=\"background-color: #fdfdff\">first</span> <span style=\"background-color: #fdfdff\">lady</span> <span style=\"background-color: #fdfdff\">peng</span> <span style=\"background-color: #fefeff\">kalra</span> <span style=\"background-color: #fcfcff\">announced</span> <span style=\"background-color: #ffffff\">friday</span> <span style=\"background-color: #fefeff\">that</span> <span style=\"background-color: #f7f7ff\">the</span> <span style=\"background-color: #f3f3ff\">panda</span> <span style=\"background-color: #fefeff\">born</span> <span style=\"background-color: #fdfdff\">last</span> <span style=\"background-color: #f6f6ff\">month</span> <span style=\"background-color: #fdfdff\">in</span> <span style=\"background-color: #fefeff\">washington</span> <span style=\"background-color: #fafaff\">,</span> <span style=\"background-color: #fefeff\">d.c</span> <span style=\"background-color: #ffffff\">.</span> <span style=\"background-color: #ffffff\">,</span> <span style=\"background-color: #fefeff\">has</span> <span style=\"background-color: #f0f0ff\">been</span> <span style=\"background-color: #ffffff\">named</span> <span style=\"background-color: #fcfcff\">&quot;</span> <span style=\"background-color: #fafaff\">bei</span> <span style=\"background-color: #ffffff\">bei</span> <span style=\"background-color: #ffffff\">,</span> <span style=\"background-color: #ffffff\">&quot;</span> <span style=\"background-color: #fdfdff\">which</span> <span style=\"background-color: #ffffff\">means</span> <span style=\"background-color: #ffffff\">&quot;</span> <span style=\"background-color: #fcfcff\">precious</span> <span style=\"background-color: #fcfcff\">treasure</span> <span style=\"background-color: #f7f7ff\">,</span> <span style=\"background-color: #ffffff\">&quot;</span> <span style=\"background-color: #ffffff\">according</span> <span style=\"background-color: #fefeff\">to</span> <span style=\"background-color: #fdfdff\">the</span> <span style=\"background-color: #fefeff\">smithsonian</span> <span style=\"background-color: #ffffff\">&apos;s</span> <span style=\"background-color: #fefeff\">national</span> <span style=\"background-color: #ffffff\">zoo</span> <span style=\"background-color: #fbfbff\">.</span> <span style=\"background-color: #fefeff\">chinese</span> <span style=\"background-color: #fefeff\">president</span> <span style=\"background-color: #fefeff\">xi</span> <span style=\"background-color: #ffffff\">jinping</span> <span style=\"background-color: #fdfdff\">has</span> <span style=\"background-color: #fefeff\">faced</span> <span style=\"background-color: #fdfdff\">a</span> <span style=\"background-color: #fdfdff\">barrage</span> <span style=\"background-color: #fefeff\">of</span> <span style=\"background-color: #fcfcff\">criticism</span> <span style=\"background-color: #ffffff\">from</span> <span style=\"background-color: #ffffff\">u.s.</span> <span style=\"background-color: #fdfdff\">presidential</span> <span style=\"background-color: #fefeff\">hopefuls</span> <span style=\"background-color: #fcfcff\">recently</span> <span style=\"background-color: #ffffff\">,</span> <span style=\"background-color: #fefeff\">~</span> <span style=\"background-color: #fcfcff\">~</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heat(sample, weights[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <span style=\"background-color: #b8b8ff\">president</span> <span style=\"background-color: #4c4cff\">barack</span> <span style=\"background-color: #8d8dff\">obama</span> <span style=\"background-color: #4c4cff\">said</span> <span style=\"background-color: #dedeff\">friday</span> <span style=\"background-color: #ededff\">that</span> <span style=\"background-color: #4c4cff\">china</span> <span style=\"background-color: #fdfdff\">and</span> <span style=\"background-color: #f2f2ff\">the</span> <span style=\"background-color: #5555ff\">united</span> <span style=\"background-color: #100100ff\">states</span> <span style=\"background-color: #fdfdff\">had</span> <span style=\"background-color: #ffffff\">agreed</span> <span style=\"background-color: #a6a6ff\">to</span> <span style=\"background-color: #100100ff\">work</span> <span style=\"background-color: #5858ff\">to</span> <span style=\"background-color: #100100ff\">avoid</span> <span style=\"background-color: #a6a6ff\">military</span> <span style=\"background-color: #4c4cff\">misunderstanding</span> <span style=\"background-color: #7070ff\">and</span> <span style=\"background-color: #a6a6ff\">desist</span> <span style=\"background-color: #a6a6ff\">from</span> <span style=\"background-color: #4c4cff\"><0>^</span> <span style=\"background-color: #bdbdff\">for</span> <span style=\"background-color: #4c4cff\">commercial</span> <span style=\"background-color: #100100ff\">gain</span> <span style=\"background-color: #4c4cff\">during</span> <span style=\"background-color: #a6a6ff\">a</span> <span style=\"background-color: #ffffff\">state</span> <span style=\"background-color: #7474ff\">visit</span> <span style=\"background-color: #ffffff\">with</span> <span style=\"background-color: #4d4dff\">chinese</span> <span style=\"background-color: #5454ff\">president</span> <span style=\"background-color: #100100ff\">xi</span> <span style=\"background-color: #100100ff\">jinping</span> <span style=\"background-color: #ababff\">.</span> <span style=\"background-color: #4c4cff\">first</span> <span style=\"background-color: #4c4cff\">lady</span> <span style=\"background-color: #4c4cff\">michelle</span> <span style=\"background-color: #ffffff\">obama</span> <span style=\"background-color: #100100ff\">and</span> <span style=\"background-color: #a6a6ff\">china</span> <span style=\"background-color: #100100ff\">&apos;s</span> <span style=\"background-color: #4c4cff\">first</span> <span style=\"background-color: #4c4cff\">lady</span> <span style=\"background-color: #100100ff\">peng</span> <span style=\"background-color: #a6a6ff\">kalra</span> <span style=\"background-color: #a6a6ff\">announced</span> <span style=\"background-color: #a6a6ff\">friday</span> <span style=\"background-color: #4c4cff\">that</span> <span style=\"background-color: #4c4cff\">the</span> <span style=\"background-color: #a6a6ff\">panda</span> <span style=\"background-color: #a6a6ff\">born</span> <span style=\"background-color: #100100ff\">last</span> <span style=\"background-color: #a6a6ff\">month</span> <span style=\"background-color: #4c4cff\">in</span> <span style=\"background-color: #6666ff\">washington</span> <span style=\"background-color: #a6a6ff\">,</span> <span style=\"background-color: #4c4cff\">d.c</span> <span style=\"background-color: #a6a6ff\">.</span> <span style=\"background-color: #100100ff\">,</span> <span style=\"background-color: #100100ff\">has</span> <span style=\"background-color: #100100ff\">been</span> <span style=\"background-color: #7878ff\">named</span> <span style=\"background-color: #100100ff\">&quot;</span> <span style=\"background-color: #a6a6ff\">bei</span> <span style=\"background-color: #100100ff\">bei</span> <span style=\"background-color: #100100ff\">,</span> <span style=\"background-color: #4c4cff\">&quot;</span> <span style=\"background-color: #100100ff\">which</span> <span style=\"background-color: #100100ff\">means</span> <span style=\"background-color: #a6a6ff\">&quot;</span> <span style=\"background-color: #100100ff\">precious</span> <span style=\"background-color: #4c4cff\">treasure</span> <span style=\"background-color: #4c4cff\">,</span> <span style=\"background-color: #4c4cff\">&quot;</span> <span style=\"background-color: #a6a6ff\">according</span> <span style=\"background-color: #100100ff\">to</span> <span style=\"background-color: #100100ff\">the</span> <span style=\"background-color: #100100ff\">smithsonian</span> <span style=\"background-color: #100100ff\">&apos;s</span> <span style=\"background-color: #100100ff\">national</span> <span style=\"background-color: #4c4cff\">zoo</span> <span style=\"background-color: #a6a6ff\">.</span> <span style=\"background-color: #100100ff\">chinese</span> <span style=\"background-color: #4c4cff\">president</span> <span style=\"background-color: #ffffff\">xi</span> <span style=\"background-color: #4c4cff\">jinping</span> <span style=\"background-color: #4c4cff\">has</span> <span style=\"background-color: #4c4cff\">faced</span> <span style=\"background-color: #100100ff\">a</span> <span style=\"background-color: #100100ff\">barrage</span> <span style=\"background-color: #100100ff\">of</span> <span style=\"background-color: #a6a6ff\">criticism</span> <span style=\"background-color: #a6a6ff\">from</span> <span style=\"background-color: #a6a6ff\">u.s.</span> <span style=\"background-color: #a6a6ff\">presidential</span> <span style=\"background-color: #a6a6ff\">hopefuls</span> <span style=\"background-color: #4c4cff\">recently</span> <span style=\"background-color: #f0f0ff\">,</span> <span style=\"background-color: #4c4cff\">~</span> <span style=\"background-color: #100100ff\">~</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heat(sample, tweights[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [idx2word[data[0,i]] for i in range(startd,maxlend)]\n",
    "rows = [idx2word[data[0,i]] for i in range(maxlend+1,maxlend+lenh+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.99473048e-07   7.61245564e-02   1.59919281e-02 ...,   5.55709528e-04\n",
      "    2.86300131e-03   1.64231844e-02]\n",
      " [  6.99473333e-07   7.61244446e-02   1.59919094e-02 ...,   5.55710052e-04\n",
      "    2.86300667e-03   1.64232161e-02]]\n",
      "100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (512, 2), indices imply (100, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   4293\u001b[0m                 blocks = [make_block(values=blocks[0],\n\u001b[1;32m-> 4294\u001b[1;33m                                      placement=slice(0, len(axes[0])))]\n\u001b[0m\u001b[0;32m   4295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[0;32m   2718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2719\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[0;32m    114\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[1;32m--> 115\u001b[1;33m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 512, placement implies 100",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-330-c5bb34f97643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlenh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlenh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[1;32m--> 306\u001b[1;33m                                          copy=copy)\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[1;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   4301\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4302\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4303\u001b[1;33m         \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HILA\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   4278\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4279\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[1;32m-> 4280\u001b[1;33m         passed, implied))\n\u001b[0m\u001b[0;32m   4281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (512, 2), indices imply (100, 2)"
     ]
    }
   ],
   "source": [
    "#df = pd.DataFrame(weights[0,:lenh,startd:],columns=columns,index=rows)\n",
    "print((weights[0,:lenh,startd:]))\n",
    "print(len(columns))\n",
    "df = pd.DataFrame(tweights[0,:lenh,startd:],columns=columns,index=rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
