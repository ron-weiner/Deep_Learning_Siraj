{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import data_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc5 in position 5: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1d7b19fc03fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#i couldn't understand why it isn't working here so i made a local function for that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_word2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_idx2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_word2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_idx2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\HILA\\Google Drive\\code\\tf\\11 - text translator seq2seq LSTM RNN\\data_utils.py\u001b[0m in \u001b[0;36mread_dataset\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc5 in position 5: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "#i couldn't understand why it isn't working here so i made a local function for that\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab = data_utils.read_dataset('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def read_dataset_latin2(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "         return pickle.load(f,encoding='latin2')\n",
    "        \n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, de_word2idx, de_idx2word, de_vocab = read_dataset_latin2('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [108, 5, 867, 93, 38, 25, 2583]\n",
      "Sentence in German - encoded: [166, 262, 8, 474, 268, 324, 67, 15, 130]\n",
      "Decoded:\n",
      "------------------------\n",
      "They\n",
      "walk\n",
      "in\n",
      "here\n",
      "and\n",
      "\n",
      "\n",
      "Die\n",
      "kommen\n",
      "hier\n",
      "herein\n",
      "und\n"
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "print('Sentence in English - encoded:', X[0])\n",
    "print('Sentence in German - encoded:', Y[0])\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[1])):\n",
    "    print(en_idx2word[X[1][i]],)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[1])):\n",
    "    print(de_idx2word[Y[1][i]],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [de_word2idx['<go>']] + y[i] + [de_word2idx['<eos>']] + (length-len(y[i])) * [de_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 15\n",
    "output_seq_len = 17\n",
    "en_vocab_size = len(en_vocab) + 2 # + <pad>, <ukn>\n",
    "de_vocab_size = len(de_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = de_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = de_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = de_word2idx['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == de_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(de_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "steps = 1000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 9.153573989868164\n",
      "step: 4, loss: 9.274894714355469\n",
      "step: 9, loss: 9.077409744262695\n",
      "step: 14, loss: 8.988450050354004\n",
      "step: 19, loss: 8.902165412902832\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 9.014216423034668\n",
      "step: 29, loss: 8.8914213180542\n",
      "step: 34, loss: 8.721532821655273\n",
      "step: 39, loss: 8.29378890991211\n",
      "Checkpoint is saved\n",
      "step: 44, loss: 7.44996452331543\n",
      "step: 49, loss: 7.462812423706055\n",
      "step: 54, loss: 6.998714447021484\n",
      "step: 59, loss: 6.7863264083862305\n",
      "Checkpoint is saved\n",
      "step: 64, loss: 6.419709205627441\n",
      "step: 69, loss: 6.344605445861816\n",
      "step: 74, loss: 6.21299934387207\n",
      "step: 79, loss: 5.753926753997803\n",
      "Checkpoint is saved\n",
      "step: 84, loss: 5.591611385345459\n",
      "step: 89, loss: 5.907456398010254\n",
      "step: 94, loss: 6.399724006652832\n",
      "step: 99, loss: 6.2806854248046875\n",
      "Checkpoint is saved\n",
      "step: 104, loss: 5.182126998901367\n",
      "step: 109, loss: 6.766603469848633\n",
      "step: 114, loss: 5.3260498046875\n",
      "step: 119, loss: 5.733614921569824\n",
      "Checkpoint is saved\n",
      "step: 124, loss: 5.135857582092285\n",
      "step: 129, loss: 5.872040271759033\n",
      "step: 134, loss: 4.786136150360107\n",
      "step: 139, loss: 4.833551406860352\n",
      "Checkpoint is saved\n",
      "step: 144, loss: 5.0055012702941895\n",
      "step: 149, loss: 5.409707069396973\n",
      "step: 154, loss: 4.786494255065918\n",
      "step: 159, loss: 5.386903762817383\n",
      "Checkpoint is saved\n",
      "step: 164, loss: 4.677248477935791\n",
      "step: 169, loss: 4.720800876617432\n",
      "step: 174, loss: 4.533707618713379\n",
      "step: 179, loss: 4.768439769744873\n",
      "Checkpoint is saved\n",
      "step: 184, loss: 4.452966213226318\n",
      "step: 189, loss: 4.726731300354004\n",
      "step: 194, loss: 4.5881428718566895\n",
      "step: 199, loss: 4.249204635620117\n",
      "Checkpoint is saved\n",
      "step: 204, loss: 5.071386814117432\n",
      "step: 209, loss: 4.377031326293945\n",
      "step: 214, loss: 4.221985816955566\n",
      "step: 219, loss: 4.453510284423828\n",
      "Checkpoint is saved\n",
      "step: 224, loss: 4.483537673950195\n",
      "step: 229, loss: 4.222024917602539\n",
      "step: 234, loss: 4.128881931304932\n",
      "step: 239, loss: 3.7155370712280273\n",
      "Checkpoint is saved\n",
      "step: 244, loss: 4.085076332092285\n",
      "step: 249, loss: 3.8913328647613525\n",
      "step: 254, loss: 3.4895617961883545\n",
      "step: 259, loss: 4.022771835327148\n",
      "Checkpoint is saved\n",
      "step: 264, loss: 3.4893152713775635\n",
      "step: 269, loss: 3.7907474040985107\n",
      "step: 274, loss: 3.531355381011963\n",
      "step: 279, loss: 3.665379762649536\n",
      "Checkpoint is saved\n",
      "step: 284, loss: 3.4090795516967773\n",
      "step: 289, loss: 3.302323818206787\n",
      "step: 294, loss: 3.675340175628662\n",
      "step: 299, loss: 2.993354320526123\n",
      "Checkpoint is saved\n",
      "step: 304, loss: 3.2448225021362305\n",
      "step: 309, loss: 3.903296709060669\n",
      "step: 314, loss: 3.0948359966278076\n",
      "step: 319, loss: 3.2380049228668213\n",
      "Checkpoint is saved\n",
      "step: 324, loss: 3.1115126609802246\n",
      "step: 329, loss: 3.5438766479492188\n",
      "step: 334, loss: 2.9052557945251465\n",
      "step: 339, loss: 3.0532495975494385\n",
      "Checkpoint is saved\n",
      "step: 344, loss: 3.2004733085632324\n",
      "step: 349, loss: 3.164950370788574\n",
      "step: 354, loss: 3.205881357192993\n",
      "step: 359, loss: 2.777991533279419\n",
      "Checkpoint is saved\n",
      "step: 364, loss: 3.052899122238159\n",
      "step: 369, loss: 2.9332475662231445\n",
      "step: 374, loss: 2.3753821849823\n",
      "step: 379, loss: 2.5510082244873047\n",
      "Checkpoint is saved\n",
      "step: 384, loss: 2.782960891723633\n",
      "step: 389, loss: 3.161207675933838\n",
      "step: 394, loss: 2.7299561500549316\n",
      "step: 399, loss: 2.735243558883667\n",
      "Checkpoint is saved\n",
      "step: 404, loss: 2.845852851867676\n",
      "step: 409, loss: 2.9375412464141846\n",
      "step: 414, loss: 2.6462419033050537\n",
      "step: 419, loss: 2.314030647277832\n",
      "Checkpoint is saved\n",
      "step: 424, loss: 2.862748622894287\n",
      "step: 429, loss: 2.259068727493286\n",
      "step: 434, loss: 2.724370002746582\n",
      "step: 439, loss: 2.6258444786071777\n",
      "Checkpoint is saved\n",
      "step: 444, loss: 2.2934744358062744\n",
      "step: 449, loss: 2.6452698707580566\n",
      "step: 454, loss: 2.6496872901916504\n",
      "step: 459, loss: 2.1465647220611572\n",
      "Checkpoint is saved\n",
      "step: 464, loss: 2.1700334548950195\n",
      "step: 469, loss: 2.3017358779907227\n",
      "step: 474, loss: 2.4193532466888428\n",
      "step: 479, loss: 2.202880620956421\n",
      "Checkpoint is saved\n",
      "step: 484, loss: 2.2114710807800293\n",
      "step: 489, loss: 2.3512330055236816\n",
      "step: 494, loss: 2.210780143737793\n",
      "step: 499, loss: 2.2584753036499023\n",
      "Checkpoint is saved\n",
      "step: 504, loss: 2.108572006225586\n",
      "step: 509, loss: 2.232112407684326\n",
      "step: 514, loss: 1.9646378755569458\n",
      "step: 519, loss: 1.9804812669754028\n",
      "Checkpoint is saved\n",
      "step: 524, loss: 2.782248020172119\n",
      "step: 529, loss: 1.9546420574188232\n",
      "step: 534, loss: 2.0973920822143555\n",
      "step: 539, loss: 1.6514455080032349\n",
      "Checkpoint is saved\n",
      "step: 544, loss: 2.336482286453247\n",
      "step: 549, loss: 1.8558433055877686\n",
      "step: 554, loss: 2.0937538146972656\n",
      "step: 559, loss: 1.879533052444458\n",
      "Checkpoint is saved\n",
      "step: 564, loss: 1.9342260360717773\n",
      "step: 569, loss: 2.013014793395996\n",
      "step: 574, loss: 1.845000982284546\n",
      "step: 579, loss: 2.2071011066436768\n",
      "Checkpoint is saved\n",
      "step: 584, loss: 1.78857421875\n",
      "step: 589, loss: 2.3164525032043457\n",
      "step: 594, loss: 1.7203454971313477\n",
      "step: 599, loss: 1.676938772201538\n",
      "Checkpoint is saved\n",
      "step: 604, loss: 1.5635411739349365\n",
      "step: 609, loss: 1.9795397520065308\n",
      "step: 614, loss: 1.7811918258666992\n",
      "step: 619, loss: 1.8407824039459229\n",
      "Checkpoint is saved\n",
      "step: 624, loss: 1.7381329536437988\n",
      "step: 629, loss: 1.8855347633361816\n",
      "step: 634, loss: 1.7690068483352661\n",
      "step: 639, loss: 1.823408842086792\n",
      "Checkpoint is saved\n",
      "step: 644, loss: 1.8340684175491333\n",
      "step: 649, loss: 1.9899570941925049\n",
      "step: 654, loss: 1.7442901134490967\n",
      "step: 659, loss: 1.610499382019043\n",
      "Checkpoint is saved\n",
      "step: 664, loss: 1.773530125617981\n",
      "step: 669, loss: 1.5591691732406616\n",
      "step: 674, loss: 1.5956491231918335\n",
      "step: 679, loss: 2.0857739448547363\n",
      "Checkpoint is saved\n",
      "step: 684, loss: 1.4098427295684814\n",
      "step: 689, loss: 1.684814214706421\n",
      "step: 694, loss: 1.5310474634170532\n",
      "step: 699, loss: 1.7829939126968384\n",
      "Checkpoint is saved\n",
      "step: 704, loss: 1.5809847116470337\n",
      "step: 709, loss: 1.6444835662841797\n",
      "step: 714, loss: 1.6707208156585693\n",
      "step: 719, loss: 2.112260341644287\n",
      "Checkpoint is saved\n",
      "step: 724, loss: 1.4441375732421875\n",
      "step: 729, loss: 1.5860220193862915\n",
      "step: 734, loss: 1.2949923276901245\n",
      "step: 739, loss: 1.3839445114135742\n",
      "Checkpoint is saved\n",
      "step: 744, loss: 1.694671630859375\n",
      "step: 749, loss: 1.3674664497375488\n",
      "step: 754, loss: 1.331611156463623\n",
      "step: 759, loss: 1.8573812246322632\n",
      "Checkpoint is saved\n",
      "step: 764, loss: 1.565036416053772\n",
      "step: 769, loss: 1.303856372833252\n",
      "step: 774, loss: 1.350830316543579\n",
      "step: 779, loss: 1.5179996490478516\n",
      "Checkpoint is saved\n",
      "step: 784, loss: 1.384694218635559\n",
      "step: 789, loss: 1.592602014541626\n",
      "step: 794, loss: 1.4381582736968994\n",
      "step: 799, loss: 1.4996540546417236\n",
      "Checkpoint is saved\n",
      "step: 804, loss: 1.1517176628112793\n",
      "step: 809, loss: 1.436251163482666\n",
      "step: 814, loss: 1.0836963653564453\n",
      "step: 819, loss: 1.3524880409240723\n",
      "Checkpoint is saved\n",
      "step: 824, loss: 1.3960325717926025\n",
      "step: 829, loss: 1.3448315858840942\n",
      "step: 834, loss: 1.2059136629104614\n",
      "step: 839, loss: 1.3091387748718262\n",
      "Checkpoint is saved\n",
      "step: 844, loss: 1.296325445175171\n",
      "step: 849, loss: 1.5885605812072754\n",
      "step: 854, loss: 1.5888316631317139\n",
      "step: 859, loss: 1.4062151908874512\n",
      "Checkpoint is saved\n",
      "step: 864, loss: 1.127677083015442\n",
      "step: 869, loss: 1.2363592386245728\n",
      "step: 874, loss: 0.9893125295639038\n",
      "step: 879, loss: 1.444327473640442\n",
      "Checkpoint is saved\n",
      "step: 884, loss: 1.0735797882080078\n",
      "step: 889, loss: 1.1921406984329224\n",
      "step: 894, loss: 1.215964913368225\n",
      "step: 899, loss: 1.312960147857666\n",
      "Checkpoint is saved\n",
      "step: 904, loss: 1.2575565576553345\n",
      "step: 909, loss: 1.0635615587234497\n",
      "step: 914, loss: 0.9832282066345215\n",
      "step: 919, loss: 1.1461676359176636\n",
      "Checkpoint is saved\n",
      "step: 924, loss: 1.1073310375213623\n",
      "step: 929, loss: 1.2272813320159912\n",
      "step: 934, loss: 1.2047390937805176\n",
      "step: 939, loss: 1.4168224334716797\n",
      "Checkpoint is saved\n",
      "step: 944, loss: 1.304608941078186\n",
      "step: 949, loss: 1.4448890686035156\n",
      "step: 954, loss: 0.9751747846603394\n",
      "step: 959, loss: 1.2639055252075195\n",
      "Checkpoint is saved\n",
      "step: 964, loss: 1.0707027912139893\n",
      "step: 969, loss: 1.072148323059082\n",
      "step: 974, loss: 0.9833293557167053\n",
      "step: 979, loss: 0.9753016233444214\n",
      "Checkpoint is saved\n",
      "step: 984, loss: 1.2900512218475342\n",
      "step: 989, loss: 1.1842842102050781\n",
      "step: 994, loss: 1.0203005075454712\n",
      "step: 999, loss: 1.0822031497955322\n",
      "Checkpoint is saved\n",
      "Training time for 1000 steps: 5359.9993777275085s\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 20 == 19:\n",
    "            saver.save(sess, 'checkpoints/', global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEkCAYAAAChew9BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw7E5bAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253682c8748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\-999\n",
      "1.\n",
      "--------------------------------\n",
      "What' s your name\n",
      "Was\n",
      "hast\n",
      "du\n",
      "\n",
      "--------------------------------\n",
      "2.\n",
      "--------------------------------\n",
      "My name is\n",
      "Ich\n",
      "kann\n",
      "nicht\n",
      "\n",
      "--------------------------------\n",
      "3.\n",
      "--------------------------------\n",
      "What are you doing\n",
      "Was\n",
      "machst\n",
      "du\n",
      "da\n",
      "\n",
      "--------------------------------\n",
      "4.\n",
      "--------------------------------\n",
      "I am reading a book\n",
      "Ich\n",
      "hab\n",
      "ein\n",
      "Buch\n",
      "\n",
      "--------------------------------\n",
      "5.\n",
      "--------------------------------\n",
      "How are you\n",
      "Wie\n",
      "geht'\n",
      "du\n",
      "\n",
      "--------------------------------\n",
      "6.\n",
      "--------------------------------\n",
      "I am good\n",
      "Ich\n",
      "bin\n",
      "gut\n",
      "\n",
      "--------------------------------\n",
      "7.\n",
      "--------------------------------\n",
      "Do you speak English\n",
      "Hast\n",
      "du\n",
      "etwas\n",
      "\n",
      "--------------------------------\n",
      "8.\n",
      "--------------------------------\n",
      "What time is it\n",
      "spĂ¤t\n",
      "spĂ¤t\n",
      "ist\n",
      "es\n",
      "\n",
      "--------------------------------\n",
      "9.\n",
      "--------------------------------\n",
      "Hi\n",
      "Hi\n",
      "\n",
      "--------------------------------\n",
      "10.\n",
      "--------------------------------\n",
      "Goodbye\n",
      "Menschen\n",
      "\n",
      "--------------------------------\n",
      "11.\n",
      "--------------------------------\n",
      "Yes\n",
      "Ja\n",
      "\n",
      "--------------------------------\n",
      "12.\n",
      "--------------------------------\n",
      "No\n",
      "Nein\n",
      "\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [de_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [de_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = de_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    en_sentences = [\"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
    "                    'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
    "    en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([de_word2idx['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print(en_sentences[i])\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print(words[i],)\n",
    "            \n",
    "            print('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model can be improved by using more training steps, better dataset or even with better selection of hyperparameters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
